"""Search Agent using DuckDuckGo MCP Server for web search capabilities."""
import asyncio
from typing import Dict, Any, Optional, List
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from mcp_client_proper import DuckDuckGoMCPClient
from config import config


class SearchAgentMCP:
    """
    Agent for performing web searches using DuckDuckGo MCP Server.
    Follows the same pattern as CryptoAgentMCP, RAGAgentMCP, and StockAgentMCP.
    """

    def __init__(self):
        """Initialize the DuckDuckGo search agent."""
        self.mcp_client = DuckDuckGoMCPClient()
        self.llm = None
        self.initialized = False

    async def initialize(self):
        """Initialize the agent with MCP client and LLM."""
        if not self.initialized:
            print("üîç Initializing Search Agent (MCP)...")
            
            # Connect to DuckDuckGo MCP Server
            print("  üì° Connecting to DuckDuckGo MCP Server...")
            success = await self.mcp_client.connect()
            
            if not success:
                raise RuntimeError("Failed to connect to DuckDuckGo MCP Server")
            
            # Initialize LLM
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                temperature=0.3,
                google_api_key=config.GOOGLE_API_KEY,
            )
            
            # Get and bind MCP tools
            tools_for_gemini = self.mcp_client.get_tools_for_gemini()
            
            # Bind tools to LLM - tools_for_gemini is already in correct format
            if tools_for_gemini:
                self.llm = self.llm.bind_tools(tools_for_gemini)
                print(f"  ‚úÖ Bound {len(tools_for_gemini)} tools to LLM")
            
            self.initialized = True
            print("  ‚úÖ Search Agent (MCP) ready!")

    async def process(
        self,
        query: str,
        history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Process a search query through the DuckDuckGo MCP server.

        Args:
            query: User's search query
            history: Optional conversation history

        Returns:
            Dictionary with agent response
        """
        try:
            if not self.initialized:
                await self.initialize()

            # Build system prompt that explicitly instructs tool usage
            system_prompt = """You are a web search assistant with access to DuckDuckGo search.

CRITICAL: You MUST use the 'search' tool to find information. DO NOT try to answer from memory.

Your process:
1. Use the 'search' tool with the user's query (limit max_results to 5)
2. Read the search results
3. Synthesize a clear answer with source citations

Available tools:
- search(query: str, max_results: int = 5): Search DuckDuckGo for information

Always use the search tool first before answering."""

            # Prepare messages
            messages = [SystemMessage(content=system_prompt)]
            
            # Add conversation history if provided (limit to last 2 turns)
            if history:
                for turn in history[-2:]:
                    if "user" in turn:
                        messages.append(HumanMessage(content=turn["user"]))
                    if "assistant" in turn:
                        messages.append(AIMessage(content=turn["assistant"]))
            
            # Add current query with explicit instruction
            messages.append(HumanMessage(
                content=f"Use the search tool to find information about: {query}"
            ))

            # Get initial LLM response with tool calls
            print(f"  üì§ Invoking LLM with search query...")
            response = await self.llm.ainvoke(messages)
            
            # Check for tool calls
            tool_calls = getattr(response, 'tool_calls', None) or []
            
            if not tool_calls:
                print(f"  ‚ö†Ô∏è No tool calls generated by LLM")
                print(f"  Response content: {response.content[:200]}...")
                return {
                    "success": False,
                    "error": "LLM did not generate tool calls",
                    "response": f"I wasn't able to search for that information. Response: {response.content}"
                }
            
            print(f"  ‚úÖ LLM generated {len(tool_calls)} tool call(s)")
            
            # Process the first tool call
            tool_call = tool_calls[0]
            
            # Handle both dict and object formats
            if isinstance(tool_call, dict):
                tool_name = tool_call.get('name', '')
                tool_args = tool_call.get('args', {})
            else:
                tool_name = getattr(tool_call, 'name', '')
                tool_args = getattr(tool_call, 'args', {})
            
            print(f"  üîß Executing: {tool_name}")
            print(f"     Arguments: {tool_args}")
            
            # Set reasonable max_results limit
            if tool_name == 'search' and 'max_results' not in tool_args:
                tool_args['max_results'] = 5
            
            # Call MCP tool with timeout
            try:
                tool_result = await asyncio.wait_for(
                    self.mcp_client.call_tool(tool_name, tool_args),
                    timeout=30.0
                )
                print(f"  ‚úÖ Tool executed successfully")
            except asyncio.TimeoutError:
                print(f"  ‚ö†Ô∏è Tool call timed out")
                return {
                    "success": False,
                    "error": "Search timed out",
                    "response": "The search took too long. Please try again with a more specific query."
                }
            except Exception as e:
                print(f"  ‚ùå Tool execution failed: {str(e)}")
                return {
                    "success": False,
                    "error": f"Tool execution failed: {str(e)}",
                    "response": f"I encountered an error while searching: {str(e)}"
                }
            
            # Check tool result
            if not tool_result.get("success"):
                error = tool_result.get("error", "Unknown error")
                print(f"  ‚ùå Tool returned error: {error}")
                return {
                    "success": False,
                    "error": error,
                    "response": f"Search failed: {error}"
                }
            
            # Extract and truncate result
            result_content = tool_result.get('result', 'No results')
            if isinstance(result_content, dict):
                result_content = str(result_content)
            
            if len(result_content) > 5000:
                print(f"     Truncating result: {len(result_content)} -> 5000 chars")
                result_content = result_content[:5000] + "\n\n[Results truncated]"
            
            print(f"  üìÑ Got search results ({len(result_content)} chars)")
            
            # Add tool response to conversation
            messages.append(response)
            messages.append(HumanMessage(
                content=f"Search results:\n\n{result_content}\n\nBased on these results, provide a comprehensive answer to the user's query. Cite specific sources by referencing result numbers."
            ))
            
            # Get final synthesized response
            print(f"  üì§ Generating final answer...")
            final_response = await self.llm.ainvoke(messages)
            print(f"  ‚úÖ Final answer generated")
            
            return {
                "success": True,
                "response": final_response.content,
                "tool_calls": [{
                    "tool": tool_name,
                    "arguments": tool_args,
                    "result_preview": result_content[:300] + "..." if len(result_content) > 300 else result_content
                }]
            }

        except Exception as e:
            error_msg = f"Error processing search query: {str(e)}"
            print(f"  ‚ùå {error_msg}")
            import traceback
            print(f"  Traceback: {traceback.format_exc()}")
            return {
                "success": False,
                "error": error_msg,
                "response": f"I encountered an error: {str(e)}"
            }

    async def cleanup(self):
        """Cleanup resources."""
        if self.initialized:
            await self.mcp_client.cleanup()
            self.initialized = False
            print("üßπ Search Agent cleanup complete")